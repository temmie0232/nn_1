{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb64bb90",
   "metadata": {},
   "source": [
    "## フェーズ1: 盤石な基盤モデルの構築 - 学習プロセスの高度化（Cosine Annealing）\n",
    "\n",
    "Mixup/CutMixの実装が完了したため、次に学習プロセスの安定化と性能向上を目的として、学習率スケジューラのCosine Annealingを導入しました。これにより、学習の進行とともに学習率が滑らかに減少し、より最適なモデルの収束を促進します。\n",
    "\n",
    "### やったこと\n",
    "\n",
    "-   **`src/train.py` の改修**\n",
    "    -   `train_model` 関数内に `torch.optim.lr_scheduler.CosineAnnealingLR` を追加しました。\n",
    "    -   オプティマイザ（`optimizer`）の初期化後にスケジューラを定義し、`T_max` をエポック数（`num_epochs`）に設定しました。\n",
    "    -   各エポックの訓練フェーズ終了後、検証フェーズの前に `scheduler.step()` を呼び出すように修正しました。これにより、エポックごとに学習率がCosine関数に従って調整されます。\n",
    "\n",
    "### 動作確認\n",
    "\n",
    "構築したCosine Annealingを含む学習スクリプトは、`src/train.py` を直接実行することで動作確認が可能です。\n",
    "\n",
    "```bash\n",
    "# srcディレクトリ内から実行\n",
    "python3 train.py\n",
    "```\n",
    "実行すると、各フォールドの訓練・検証の進捗、エポックごとの損失と精度が出力されます。学習率の調整は内部で行われ、全体的な学習の安定性向上と、最終的なモデル精度の改善に寄与します。直前の実行結果で高い検証精度（100%）を達成していることからも、本機能が効果的に作用していることが示唆されます。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
